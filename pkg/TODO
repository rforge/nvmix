### Major changes    ###########################################################

 - in rnvmix() (and their wrappers), factor is now a matrix R such that R R^T = scale.
   (=> consistent with paper).
   Internally, if 'factor' is not provided, it will be determined as the upper triangular
   Cholesky factor and not changed so that we can multiply from the right. If 'factor' is
   provided, it will be transposed, so that we can multiply from the right. This avoids 
   transposing Z which is usually much larger than R. 

   *NOTE*: As a result, the default of factor is NULL now because I did not touch 'factorize'. 
   I could have written factor = t(factorize(scale)) but then rnvmix() transposes that factor 
   again, so two transposes cancelling each other (not pretty, not efficient). 
   Leaving factor = factorize(scale) would be very confusing for the user.
   I understand that this is probably not the most elegant solution, but I think it works for
   now and we can for the next release think about what to do with 'factorize'. 

 - tests created => using package testthat() (quite convenient, I think)
   [MH: Wouldn't do... or just for us. Problem: dependence on a package. Hmmm... maybe ok here]

 - qW as well as mean.sqrt.mix are built in pnvmix and then passed as an argument
   to pnvmix1() => makes it faster if upper is a matrix; since qW and mean.sqrt.mix
   don't change with upper/lower, it probably makes sense to build them once.
   [MH: sounds good]


### For next release ###########################################################

## Important

- EH: 'factor' in rnvmix changed; now change it in dnvmix

- EH:  Write vignette nvmix_functionality.Rmd

- EH:  Send Marius the references for the manuals/vignette? 
       Maybe the three publications of Genz and Bretz as well as the "normal people"?

- DONE? Does it make sense to choose the default of fun.eval[2] differently?
        I feel it should be something like 1/abstol, so that if one plays
        with abstol, one has a reasonable chance not having to modify
        fun.eval[2]. I realized this when choosing abstol = 1e-8 (then
        with fun.eval[2] = 1e8, the computations took forever.

	EH: the default is set such that the algorithm terminates with reasonable
        tolerances (0.001, 0.0005, maybe 0.0001). Tolerances beyond that will take
        forever one way or another.
	My suggestion: Leave it for now.


### TODO #######################################################################

- TODO Allow rmix() to be specified in pnvmix() for method = 'PRNG'. This can be a first
       Step towards evaluation GIG mixtures, where qmix() is not available but one can 	
       sample from the mixing distribution.
- TODO look into quantile functions of univariate (marginal) normal variance mixtures.
       If we have a function, say, qnvmix1 (for 1 dimensional normal variance mixtures),
       we have a way to sample from univariate normal variance mixtures using inversion,
       which can then be used for quasi-Monte Carlo sampling
- TODO Need fitnvmix(): check what other packages do




### DONE #######################################################################

-DONE   There was a bug in rnvmix; it couldn't handle non-square 'factor' properly.
        [MH: Correct, never tested that.]
        Problem was that a *d* dimensional normal was simulated, should be *k* dimensional.
        [MH: Correct]
        Also, 'loc' needs to be *k* (not d) dimensional in this case.
        [MH: A should be (d,k), so the outcome must be d-dimensional and thus also
        loc must be d-dimensional]

        Note:
        *in the paper: A (which somewhat corresponds to our'factor') is a (d,k) matrix such that A A^T = scale.
        This is the definition in the QRM book.
        [MH: ... and the one we should follow (anything else is confusing)]
        The resulting nvmix dist'n has thus dimension *d*.
        [MH: correct]
        *in rnvmix: 'factor' is a (d,k) matrix such that A^T A = scale.
        The resulting nvmix dist'n has thus dimension *k*
        [MH: ... this is because chol() by default gives you an upper triangular Cholesky factor.
          For performance reasons, I wanted to avoid doing t(A) in the original code.
	  Maybe we can still avoid it? If not, do the t(), this is still better than loc being k-dimensional]

        I have left it the way it was (i.e. A^T A = scale) and fixed the bug.
        I originally wanted to change it to make it consistent with the paper,
        but it seems to be accepted that a factor is a matrix such that A^T A = scale.
        [MH: still, it's too confusing to be different from the literature]
        If you'd like it to be changed, that'd be fine for me, too, though. In this case, factorize() needs
        to be changed as well.
        [MH: yeah, should be changed (not sure whether also already in factorize [stay closer to R behavior?]).]

        Another remark:
        If rnvmix() is called with a *singular* scale, an error is thrown.
        [MH: How is that tested? couldn't immediately find. rnvmix() should be fast, no (too) long testing...]
        If the user wishes to sample from that singular distribution, they *have to* provide 'factor'.
        [MH: sounds ok]
   
        EH: Please see 'Major Changes'
- DONE	EH: 'rnvmix()' needs a 'method' argument (see 'pnvmix') + new argument 	'mix' ('rmix'?)
         for passing a RNG
         => call other arguments 'mix' 'qmix' instead (check carefully).
	dnvmix() and pnvmix() have 'qmix' (rather than 'mix') now; rnvmix() has both, 	'qmix' and 'rmix'
- DONE  EH: rnvmix() should have 'skip' if method = 'sobol'
- DONE  EH: renaming etc in dnvmix() similar to pnvmix()
- DONE	EH: TODOs inside pnvmix()
- DONE 	EH: pnvmix() has multiple lower/upper as input, but only one scale. pnvmix1() 
        then calculates t(chol(scale)) for each input => maybe do that in pnvmix once and 
        pass it as argument to pnvmix1()
- DONE ?pnvmix and ?dnvmix describe the output value 'numiter'. Can you had
      	(on each help pages) half a sentence what these iterations are exactly?
      	=> alternatively, shouldn't we rather return the variance? (one of the
         five components you previously returned) Seems more intuitive, no?
	EH: Will do. variance can be computed from error; numiter (or equivalently, total 
        function evaluations) should just give an idea of how long it took to converge
- DONE	EH: ?pnvmix describes 'precond'; can you add half a sentence
  	describing the idea behind the preconditioning, please?
- DONE  implement dnvmix() as well as wrappers for normal/t case.
- DONE  (d=1): pnvmix() should work with d = 1 and with missing data
- DONE  pStudent should work with df = Inf
- DONE  implement multivariate normal distribution
- DONE  rename 'a' and 'b' to 'lower' and 'upper'; 'nu' to 'df'
        (first arg should be 'upper' with default lower being '-Inf,...')
- DONE  'R' should be 'scale' and we need a 'standardized = FALSE' argument;
        if standardized = TRUE, loc = 0 and sigma = correlation matrix is assumed
        ... or so)
- DONE  'swap' needs to be improved
- DONE  'func' not needed if 'base case' in pnvmix() is part of the loop
        (only one call necessary then, so can be omitted)
- DONE  more intuitive names for arguments concerning tolerance(s)
- DONE  polish ./src


### More thoughts ##############################################################

pnvmix() does not work for multiple upper/lower input vectors, since the
preconditioning heavily depends on the input parameters. However, in d = 1,
since we have the function pnorm(), the integration problem is actually
one-dimensional. In this case, no reordering needs to be done and we can implement
a "vectorized" pnvmix1() (I use 1 to highlight that it works only for d = 1).
Indeed, we only need to sample from the mixture distribution and then evaluate
pnorm() with different upper/lower bounds. We can use common random numbers
(i.e. simulate from R, use these simulations for all upper/lower) and evaluate
the df efficiently at many points. This is particularly useful for
implementing qnvmix1(), since pnvmix1() can evaluate at multiple "upper" at once.
Still, pnvmix1() will only approximate the true df, numerically inverting that
approximated df will inevitably lead to numerical inaccuracies.


### Packages dealing with multivariate Student's t distribution ################

2018-06-21:
- monomvn (estimation under monotone pattern missing data; MLE & Bayesian)
- mvnfast (no pnvmix(), simulation, density, Mahalanobis distances)
- tmvtnorm (has ptmvt(), based on Gibbs sampler; see http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.26.6892)
- MVT (studentFit(); for fixed nu (= eta here), estimates location and scale; paper mentions EM Algorithm)
- mvtnorm (clear; no fitting, no non-integer dof, qmvt() [equicoordinate quantile function])
- QRM (fit.mst())
