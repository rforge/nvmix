---
title: Multivariate Normal Variance Mixtures
author: Erik Hintz, Marius Hofert and Christiane Lemieux
date: '`r Sys.Date()`'
output:
  html_vignette:
    css: style.css
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Multivariate Normal Variance Mixtures}
  %\VignetteEncoding{UTF-8}
---
```{r, message = FALSE}
library(nvmix)
library(scatterplot3d)
doPDF <- FALSE
```

## 1 Introduction

The R package `nvmix` provides functionality for (multivariate) normal variance mixture
distributions, including normal and Student's *t* distributions.
A random vector $\mathbf{X}=(X_1,\dots,X_d)$ follows a *normal
variance mixture*, in notation $\mathbf{X}\sim NVM_d(\mathbf{\mu},\Sigma,F_W)$, if, in
distribution,
\begin{align*}
  \mathbf{X}=\mathbf{\mu}+\sqrt{W}A\mathbf{Z},
\end{align*}
where $\mathbf{\mu}\in\mathbb{R}^d$ denotes the *location (vector)*,
$\Sigma=AA^T$ for $A\in\mathbb{R}^{d\times k}$ denotes the *scale (matrix)* (a
covariance matrix), and the mixture variable $W\sim F_W$ is a non-negative random
variable independent of $\mathbf{Z}\sim N_k(\mathbf{0},I_k)$ (where
$I_k\in\mathbb{R}^{k\times k}$ denotes the identity matrix). Note that both the
Student's $t$ distribution with degrees of freedom parameter
$\nu>0$ and the normal distribution are normal variance mixtures;
in the former case, $W\sim IG(\nu/2, \nu/2)$ (inverse gamma) and in the latter
case $W$ is almost surely constant (taken as $1$ so that $\Sigma$ is the
covariance matrix of $\mathbf{X}$ in this case).

Note that both `dnvmix()` and `pnvmix()` currently require $\Sigma$ to be
positive definite. In this case one can take $A\in\mathbb{R}^{d\times d}$ to be
the (lower triangular) Cholesky factor $L$ of $\Sigma$ such that $LL^\top=\Sigma$.
This corresponds to the argument `factor` in those functions. In `rnvmix()`,
`factor` is of the general form as $A$ above.

For most functions in the package, the *quantile function* of $W$ needs to be
provided which is (here) defined as
\begin{align*}
  F_W^\leftarrow(u)=\inf\{w\in[0,\infty):F_W(w)\ge u\},\quad u\in[0,1].
\end{align*}


## 2 Evaluating the distribution function

An important but difficult task is to evaluate the (cumulative) distribution
function of a normal variance mixture distribution, so
\begin{align*}
	F(\mathbf{x})=P(\mathbf{X}\le\mathbf{x})=P(X_1\le x_1,\dots,X_d\le x_d),\quad \mathbf{x}\in\mathbb{R}^d;
\end{align*}
note that `pnvmix()` currently accepts only positive definite `scale` matrices.
To this end, the function `pnvmix()` internally approximates the $d$-dimensional
integral using a randomized Quasi Monte-Carlo (RQMC) method. Due to the
random nature, the result depends (slightly) on the seed `.Random.seed`.

### 2.1 Exponential mixture distribution

As a first example, consider a normal variance mixture with mixture variable
$W \sim\mathrm{Exp}(`r rate`)$. We illustrate two approaches how to use
`pnvmix()` to approximate $P(\mathbf{a} < \mathbf{X} \le \mathbf{b})$
for randomly chosen $\mathbf{a}\le\mathbf{b}$.
```{r}
## Generate a random correlation matrix and random limits in dimension d = 5
d <- 5
set.seed(42)
A <- matrix(runif(d * d), ncol = d)
P <- cov2cor(A %*% t(A)) # (randomly generated) correlation matrix
b <-  3 * runif(d) * sqrt(d) # (randomly generated) upper limit
a <- -3 * runif(d) * sqrt(d) # (randomly generated) lower limit

## Specify the mixture distribution parameter
rate <- 1.9 # exponential rate parameter

## Method 1: Use R's qexp() function and provide a list as 'mix'
set.seed(42)
(p1 <- pnvmix(b, lower = a, qmix = list("exp", rate = rate), scale = P))

## Method 2: Define the quantile function manually (note that
##           we do not specify rate in the quantile function here,
##           but conveniently pass it via the ellipsis argument)
set.seed(42)
(p2 <- pnvmix(b, lower = a, qmix = function(u, lambda) -log(1-u)/lambda,
              scale = P, lambda = rate))

## Comparison
stopifnot(all.equal(p1, p2))
```
We see that the results coincide.

If higher precision of the computed probabilities is desired, this can be
accomplished by changing the argument `abstol` (which defaults to `1e-3`) at the
expense of a higher run time.
```{r}
pnvmix(b, lower = a, qmix = function(u, lambda) -log(1-u)/lambda,
       lambda = rate, scale = P, abstol = 1e-5)
```

### 2.2 Three-point mixture distribution

As a next example, consider a normal variance mixture where $W$ is discrete such
that $P(W=1)=0.2$, $P(W=3)=0.3$ and $P(W=5)=0.5$. This time, we are interested
in computing the one-sided probabability $P(\mathbf{X}\le\mathbf{b})=F(\mathbf{b})$
for $\mathbf{b}$ as constructed before.
```{r}
## Define the quantile function of the three-point distribution
qW <- function(u)
  (u <= 0.2) * 1 + (u > 0.2 & u <= 0.5) * 3 + (u > 0.5) * 5

## Call pnvmix(); lower defaults to (-Inf,...,-Inf)
pnvmix(b, qmix = qW, scale = P)

## This could have also been obtained as follows
as.numeric(0.2 * pNorm(b, scale = P) + 0.3 * pNorm(b, scale = 3 * P) +
           0.5 * pNorm(b, scale = 5 * P))
```
Note that in the latter computation, `pNorm()` (so `pnvmix()`) is called three
times, so it takes much longer in higher dimensions.

### 2.3 The wrappers `pNorm()` and `pStudent()`

As mentioned, the normal distribution as well as the $t$ distribution are
important special cases of normal variance mixture models. For these two cases,
`pnorm()` and `pStudent()` are user-friendly wrappers. Note that `pStudent()`
works for any degree of freedom parameter $\nu>0$ (not necessarily integer) -
this functionality was to the best of our knowledge not available in `R` at the
time of development of this package.

### 2.4 The effect of algorithm-specific parameters

The function `pnvmix()` (and thus the wrappers `pStudent()` and `pNorm()`) give
the user the possibility to change algorithm specific parameters. A few of them
are:
 - `method`: Integration method to be used. The default, a randomized Sobol
   sequence, has proven to outperform the others.
 - `precond`: Logical indicating wether or not a *preconditioning step*, that is
   reordering of integration limits and rows and columns of `scale`, is to be
   performed. If `TRUE`, the reordering is done in a way such that the expected
   lengths of the integration limits is increasing going from the outermost to
   the innermost integral. In the vast majority of cases, this leads to a
   decrease in the variance of the integrand, and hence to a decrease in
   computational time needed.
 - `mean.sqrt.mix`: $E(\sqrt{W})$. This number is needed for the above mentioned
   preconditioning. In case of a Normal distribution and $t$ distribution, this
   value is calculated internally. For all the other cases this value is
   estimated internally if not provided.
 - `increment`: Determines how large the next point-set should be if the
   previous point-set was not big enough to ensure the desired accuracy. When
   `"doubling"` is used, there will be as many additional points as there were
   in the previous iteration. If `"num.init"` is used, there will be
   `fun.eval[1]` additional points in each iteration. The former option
   (default) will lead to slightly more accurate results at the cost of slightly
   higher run-time.

For the others, see also `?pnvmix`. Let us illustrate the effect of `method` and
`precond` on the performance of `pnvmix()` with `mix = 'inverse.gamma'`. We can
use the wrapper `pStudent()` in this case.  We set `abstol = NULL` so that the
algorithm runs until the number of function evaluations exceeds
`fun.eval[2]`. We do this for different values of `fun.eval[2]` in order to get
an idea of the convergence speed.

```{r, fig.align = "center", fig.width = 6, fig.height = 6, fig.show = "hold", warning = FALSE}
if(doPDF) pdf(file = (file <- paste0("fig_1.pdf")), width = 6, height = 6)

## Specify degree of freedom parameter
df <- 0.1

## i iterations require 3 * 2^8 * 2^i function evaluations
maxiter <- 9
max.fun.evals <- 3 * 2^8 * 2^seq(from = 2, to = maxiter, by = 1)

errors <- matrix(NA, ncol = length(max.fun.evals), nrow = 4)

## By resetting the seed, we use the same random numbers each time
## => fairer comparison
for(i in seq_along(max.fun.evals)){
  N.max <- max.fun.evals[i]
  ## Get the corresponding errors. We set 'verbose = 0' to avoid warnings,
  ## that will inevitably occur, since we set 'abstol = NULL'

  ## Sobol with preconditioning
  set.seed(42)
  errors[1,i] <- attr( pStudent(b, lower = a, scale = P,  df = df,
                              method = "sobol", precond = TRUE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")
  ## Sobol without preconditioning
  set.seed(42)
  errors[2,i] <- attr( pStudent(b, lower = a, scale = P,  df = df,
                              method = "sobol", precond = FALSE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")
  ## PRNG with preconditioning
  set.seed(42)
  errors[3,i] <- attr( pStudent(b, lower = a, scale = P,  df = df,
                              method = "PRNG", precond = TRUE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")
  ## PRNG without preconditioning
  set.seed(42)
  errors[4,i] <- attr( pStudent(b, lower = a, scale = P,  df = df,
                              method = "PRNG", precond = FALSE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")

}

plot( max.fun.evals, errors[4,], type = "l", col = "blue", log = "xy",
      ylim= c( min(errors), max(errors)), ylab = "Estimated error",
      xlab = "Number of function evaluations")
coeff = as.numeric(lm(log(errors[4,]) ~ log(max.fun.evals))$coeff[2])
name1 <- paste("PRNG w.o. precond.,",toString(round(coeff,2)))

lines(max.fun.evals, errors[3,], col = "red")
coeff = as.numeric(lm(log(errors[3,]) ~ log(max.fun.evals))$coeff[2])
name2 <- paste("PRNG with precond.,",toString(round(coeff,2)))

lines(max.fun.evals, errors[2,], col = "green")
coeff = as.numeric(lm(log(errors[2,]) ~ log(max.fun.evals))$coeff[2])
name3 <- paste("Sobol w.o. precond.,",toString(round(coeff,2)))

lines(max.fun.evals, errors[1,], col = "black")
coeff = as.numeric(lm(log(errors[2,]) ~ log(max.fun.evals))$coeff[2])
name4 <- paste("Sobol with precond.,",toString(round(coeff,2)))

legend('topright', c(name1,name2,name3,name4),
       col = c("blue","red","green","black"), lty = rep(1,4),
       bty="n")

if(doPDF) dev.off()
```
The above plot is done on $\log-\log$ scale; the numbers in the legend are the
regression coefficients. We can see that in this example that Sobol clearly
outperforms PRNG and that the preconditioning helps significantly reduce the
error.


## 3 Evaluating the density function

Another important task is to evaluate the density function of a normal variance
mixture distribution. This is particularly important for likelihood based
methods. To this end, the function `dnvmix()` internally approximates a
one-dimensional integral using a randomized Quasi Monte-Carlo method. Due to the
random nature, the result depends (slightly) on `.Random.seed`. Note that if
$\Sigma$ is singular, the density does not exist.

Note the argument `log` in `dnvmix()`: Rather than estimating the density,
internally the *log* density is estimated and only if `log = FALSE` (the
default), the actual density is returned. This is usually more stable than
estimating the density and then applying the $\log()$ function; also note that
for many applications, interest actually lies in the log density, for instance
when approximating the log-likelihood.

We give a small example where $W$ is again such that $P(W=1)=0.3$, $P(W=3)=0.4$
and $P(W=5)=0.3$.
```{r}
## we use qW from before
## Evaluate at the following 3 points simulatenously
x <- matrix(1:15/15, ncol = d)

## Density
set.seed(1)
(d1 <- dnvmix(x, qmix = qW, scale = P))

## Log Density
set.seed(1)
(d2 <- dnvmix(x, qmix = qW, scale = P, log = TRUE))

stopifnot(all.equal(d1, exp(d2), check.attributes = FALSE))
```
The error estimate given is the *worst case* error for all the inputs, i.e. the
error of the row with the largest error. Note also how `error` is much smaller
than the default tolerance $10^{-3}$: This is due to the fact that `dnvmix()`
iterates until the desired precision is reached for *both*, the log - density
and the density. Furthermore, the error estimate for `log = FALSE` is
conservative - it will only differ from the error when `log = TRUE` if at least
one density estimate is larger than 1.


## 4 (Quasi-)random number generation

The function `rnvmix` provides a flexible tool to sample from (multivariate)
normal variance mixtures. The structure is similar to the one of `dnvmix()` and
`pnvmix()`. The user can specify the argument `qmix` (which, as usual,
corresponds to the quantile function of $W$) or, alternatively, the argument
`rmix`, which corresponds to a random number generator for $W$. The advantage is
that there may be distributions for which it is hard to find the quantile
function, but for which sampling (e.g. by acceptance/rejection or stochastic
representations) is easy.

Consider the first example, where $W\sim\mathrm{Exp}(`r rate`)$.
```{r, fig.align = "center", fig.width = 6, fig.height = 6, fig.show = "hold"}
if(doPDF) pdf(file = (file <- paste0("fig_2.pdf")), width = 6, height = 6)

## Sample size
n <- 500

## We use P = diag(2), which is the default
set.seed(42)
r1 <- rnvmix(n, rmix = list("exp", rate = rate))
plot(r1, xlab = expression(X[1]), ylab = expression(X[2]))

if(doPDF) dev.off()
```

Another important argument of `rnvmix` is `method`. This can be either `"PRNG"`
for classical random sampling or `"sobol"` or `"ghalton"` which corresponds to
inversion of a low-discrepancy pointset. If `method` is not `PRNG`, `qmix` must
be provided.

Let us revisit the example from before, where $W$ was discrete.
```{r, fig.align = "center", fig.width = 9, fig.height = 6, fig.show = "hold"}
if(doPDF) pdf(file = (file <- paste0("fig_3.pdf")), width = 9, height = 6)

r2 <- rnvmix(n, qmix = qW)
r3 <- rnvmix(n, qmix = qW, method = "ghalton")

par(mfrow=c(1,2))
plot(r2, xlab = expression(X[1]), ylab = expression(X[2]),
     main = "Pseudo-random sample", xlim = c(-7.3,7.3), ylim = c(-7.3,7.3))
plot(r3, xlab = expression(X[1]), ylab = expression(X[2]),
     main = "Quasi-random sample", xlim = c(-7.3,7.3), ylim = c(-7.3,7.3))

if(doPDF) dev.off()
```

When $W$ is discrete and has finite support, one can easily sample from the
corresponding normal variance mixture using `rNorm()` as well.
```{r, fig.align = "center", fig.width = 6, fig.height = 6, fig.show = "hold"}
if(doPDF) pdf(file = (file <- paste0("fig_3.pdf")), width = 6, height = 6)

r4.1 <- rNorm(0.3*n, scale = diag(1, 2))
r4.2 <- rNorm(0.4*n, scale = diag(3, 2))
r4.3 <- rNorm(0.3*n, scale = diag(5, 2))

plot(r4.3, col = "blue", xlab = expression(X[1]), ylab = expression(X[2]))
points(r4.2, col = "red")
points(r4.1, col = "green")
```

The green points come from $N(\mathbf{0}, I_2)$, the red ones from
$N(\mathbf{0}, 3I_2)$ and the blue ones from $N(\mathbf{0}, 5I_2)$; their
frequencies correspond to the probabilities $P(W=1)$, $P(W=3)$ and $P(W=5)$.

Unlike `pnvmix()` and `dnvmix()`, `rnvmix()` can handle *singular* normal
variance mixtures. In this case, the matrix `factor` (which is a matrix $A$ such
that $AA^T=\Sigma$) has to be provided. In the following example, we consider a
$t$ distribution; here we can use the wrapper `rStudent()`.
```{r, fig.align = "center", fig.width = 6, fig.height = 6, fig.show = "hold"}
if(doPDF) pdf(file = (file <- paste0("fig_4.pdf")), width = 6, height = 6)

## Specify degrees of freedom
df <- 3.9

## Specify factor
factor <- matrix( c(1,0,0,1,0,1), ncol = 2, byrow = TRUE)

## The corresponding 'scale' matrix is then
tcrossprod(factor)

## Now sample
set.seed(42)
r5 <- rStudent(n, df = df, factor = factor)
scatterplot3d(r5, xlab = expression(X[1]), ylab = expression(X[2]),
              zlab = expression(X[3]), angle = -40)

if(doPDF) dev.off()
```
As expected, all points lie on the same plane.








