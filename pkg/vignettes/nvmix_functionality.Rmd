---
title: Functionalities for Multivariate Normal Variance Mixture Distributions
author: Marius Hofert, Erik Hintz and Christiane Lemieux
date: '`r Sys.Date()`'
output:
  html_vignette:
    css: style.css
vignette: >
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteIndexEntry{Evaluating Multivariate Normal Mixture Distribution Functions}
  %\VignetteEncoding{UTF-8}
---
```{r, message = FALSE}
library(nvmix)
#library(plotly)
library(scatterplot3d)
doPDF <- FALSE
```



## 0 Introduction

This package provides functionalities for *Multivariate Normal Variance Mixture Distributions*. A random vector $\mathbf{X}=(X_1,\dots,X_d)$ follows a *normal variance
  mixture*, notation $\mathbf{X}\sim NVM_d(\mathbf{\mu},\Sigma,F_W)$, if, in
distribution,
$$  \mathbf{X}=\mathbf{\mu}+\sqrt{W}A\mathbf{Z},$$
where $\mathbf{\mu}\in\mathbb{R}^d$ denotes the *location (vector)*, $\Sigma=AA^T$ for
$A\in\mathbb{R}^{d\times k}$ the *scale (matrix)* (a covariance matrix), and
$W\sim F_W$ is a non-negative random variable independent of
$\mathbf{Z}\sim N_k(\mathbf{0},I_k)$ (where $I_k\in\mathbb{R}^{k\times k}$ denotes the identity
matrix); see, for example, \cite[Section~6.2]{mcneilfreyembrechts2015}.
Note that both, the multivariate Student $t$ distribution with degrees of freedom parameter $\nu>0$ and the multivariate Normal distribution are normal variance mixtures; in the former case, $W\sim IG(\nu/2, \nu/2)$ and in the latter case $W$ is a.s. constant. 


For most functions in the package, the 'quantile function' of $W$ needs to be provded; the quantile function
of $W$ is defined as
$$ F_W^\leftarrow(u)=\inf\{w\in[0,\infty):F_W(w)\ge u\}. $$

Also, all functions in the package currently require $\Sigma$ to be positive definite. In this case one can take $A\in\mathbb{R}^{d\times d}$ to be the (lower) Cholesky factor of $\Sigma$. 


## 1 Evaluating the Distribution function 

An important but difficult task is to evaluate the (cumulative) distribution function of a normal variance mixture distribution. To this end, the function `pnvmix()` internally approximates a $d$ dimensional integral using a randomized Quasi Monte-Carlo method. Due to the random nature, the result depends (slightly) on `.Random.seed`. Currently, `pnvmix()` only accepts a positive definite `scale` matrix. 

### Two small examples 
As a first example, consider a normal variance mixture where $W \sim Exp(3)$. We illustrate two approaches how to use `pnvmix` to approximate $P(\mathbf{a} \leq \mathbf{X} \leq \mathbf{b})$:

```{r}
## Generate a random correlation matrix and random limits in dimension d=5:
set.seed(42)
d <- 5
A <- matrix(runif(d * d), ncol = d)
P <- cov2cor(A %*% t(A))

b <-  3 * runif(d) * sqrt(d) # random upper limit
a <- -3 * runif(d) * sqrt(d) # random lower limit

## Specify rate
rate <- 1.9

## Method 1: use R's qexp() function => provide a list for the argument 'mix':
set.seed(42)
(p1 <- pnvmix(b, lower = a, qmix = list("exp", rate = rate), scale = P))

## Method 2: define quantile function manually:
## Note that we do not specify rate in the quantile function here,
## we can pass it conveniently using the ellipsis
set.seed(42)
(p2 <- pnvmix(b, lower = a, qmix = function(u, lambda) -log(1-u)/lambda, scale = P,
             lambda = rate))

stopifnot(all.equal(p1, p2))
```

... and we see that the results coincide. If higher precision is desired, this can be accomplished
by changing the argument `abstol` (whose default is 1e-3):
```{r}
pnvmix(b, lower = a, qmix = function(u, lambda) -log(1-u)/lambda, scale = P,
       lambda = rate, abstol = 1e-5)
```

Of course, the price to pay is that more iterations are needed, which increases run-time. 

As a next example, consider a normal variance mixture where $W$ is discrete. For instance, let $W$ such that $P(W=1)=0.3$, $P(W=3)=0.4$ and $P(W=5)=0.3$. We use the same parameters as above, but this time, we are interested in the one sided probabability $P( \mathbf{X} \leq \mathbf{b})$:

```{r}

## Define quantile function.
qW <- function(u)
  (u <= 0.3) * 1 + ( u > 0.3 & u <= 0.7) * 3 + ( u > 0.7 ) * 5

## Call pnvmix; lower defaults to (-Inf,...,-Inf)
pnvmix(b, qmix = qW, scale = P)


## This could also have been obtained as
as.numeric( 0.3 * pNorm(b, scale = P) + 0.4 * pNorm(b, scale = 3*P) +
  0.3 * pNorm(b, scale = 5*P) )
## but this calls pNorm (and hence pnvmix()) three times, hence takes much longer
## (especially in higher dimensions).
```

### The Wrappers `pNorm()` and `pStudent()`

As mentioned, the normal distribution as well as the $t$ distribution are important special cases of normal variance 
mixture models. For these two cases, `pnorm()` and `pStudent` are user-friendly wrappers. Note that `pStudent` works for any degree of freedom parameter $\nu>0$ (not necessarily integer) - this functionality was to the best of our knowledge not available in `R` at the time of development of this package. 


### The Effect of Algorithm Specific Parameters

The function `pnvmix` (and hence also the wrappers `pStudent` and `pNorm`) give the user the possibility to change algorithm specific parameters. A few of them are:

 - `method`: Integration method to be used. The default, a randomized Sobol sequence, has proven to outperform the others. 
 - `precond`: Logical indicating wether or not a *preconditioning step*, that is reordering of integration limits and rows and columns of `scale`, is to be performed. If `TRUE`, the reordering is done in a way such that the expected lengths of the integration limits is increasing going from the outermost to the innermost integral. In the vast majority of cases, this leads to a significant decrease in variance, and hence decrease in CPU.
 - `mean.sqrt.mix`: $E(\sqrt{W})$. This number is needed for the above mentioned preconditioning. In case of a Normal distribution and $t$ distribution, this value is calculated internally. For all the other cases, if the value is not provided, it is estimated.
 - `increment`: Determines how large the next point-set should be if the previous point-set was not big enough to ensure the desired accuracy. When `"doubling"` is used, there will be as many additional points as there were in the previous iteration. If `"num.init"` is used, there will be `fun.eval[1]` additional points in each iteration. The former option (default) will lead to slightly more accurate results at the cost of slightly higher run-time.

For the others, see also `?pnvmix`. Let us illustrate the effect of `method` and `precond` on the performance. 
We set `abstol = NULL` so that the algorithm runs until the number of function evaluations exceeds `fun.eval[2]`. We do this for different values of `fun.eval[2]` in order to get an idea of the convergence speed. 

```{r, fig.align = "center", fig.width = 6, fig.height = 6, fig.show = "hold", warning=FALSE}
if(doPDF) pdf(file = (file <- paste0("fig_.pdf")), width = 7, height = 7)

## i iterations require 3 * 2^8 * 2^i function evaluations 
maxiter <- 9
max.fun.evals <- 3 * 2^8 * 2^seq(from = 1, to = maxiter, by = 1) 

errors <- matrix(NA, ncol = length(max.fun.evals), nrow = 4)

for(i in seq_along(max.fun.evals)){
  N.max <- max.fun.evals[i]
  ## Get the corresponding errors. We set 'verbose = 0' to avoid warnings,
  ## that will inevitably occur, since we set 'abstol = NULL'
  
  ## Sobol with preconditioning:
  set.seed(42)
  errors[1,i] <- attr( pnvmix(b, qmix = "inverse.gamma", scale = P,  df = 0.9,
                              method = "sobol", precond = TRUE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")
  ## Sobol without preconditioning:
  set.seed(42)
  errors[2,i] <- attr( pnvmix(b, qmix = "inverse.gamma", scale = P,  df = 0.9,
                              method = "sobol", precond = FALSE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")
  ## PRNG with preconditioning:
  set.seed(42)
  errors[3,i] <- attr( pnvmix(b, qmix = "inverse.gamma", scale = P,  df = 0.9,
                              method = "PRNG", precond = TRUE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")
  ## PRNG without preconditioning:
  set.seed(42)
  errors[4,i] <- attr( pnvmix(b, qmix = "inverse.gamma", scale = P,  df = 0.9,
                              method = "PRNG", precond = FALSE, abstol = NULL,
                              fun.eval = c(2^6, N.max), verbose = FALSE), "error")

}

plot( max.fun.evals, errors[4,], type = "l", col = "blue", log = "xy",
      ylim= c( min(errors), max(errors)), ylab = "Estimated Error",
      xlab = "Number of function evaluations")
coeff = as.numeric(lm(log(errors[4,]) ~ log(max.fun.evals))$coeff[2])
name1 <- paste("PRNG w.o. precond.,",toString(round(coeff,2)))

lines(max.fun.evals, errors[3,], col = "red")
coeff = as.numeric(lm(log(errors[3,]) ~ log(max.fun.evals))$coeff[2])
name2 <- paste("PRNG with precond.,",toString(round(coeff,2)))

lines(max.fun.evals, errors[2,], col = "green")
coeff = as.numeric(lm(log(errors[2,]) ~ log(max.fun.evals))$coeff[2])
name3 <- paste("Sobol w.o. precond.,",toString(round(coeff,2)))

lines(max.fun.evals, errors[1,], col = "black")
coeff = as.numeric(lm(log(errors[2,]) ~ log(max.fun.evals))$coeff[2])
name4 <- paste("Sobol with precond.,",toString(round(coeff,2)))

legend('topright', c(name1,name2,name3,name4),
       col = c("blue","red","green","black"), lty = rep(1,4),
       bty="n")

if(doPDF) dev.off()
```

The above plot is done on $\log-\log$ scale; the numbers in the legend are the regression coefficients. We can see that in this example that Sobol clearly outperforms PRNG and that the preconditioning helps significantly reduce the error. 


## 2 Evaluating the Density Function 

Another important task is to evaluate the density function of a normal variance mixture distribution. This is particularly important for likelihood based methods. To this end, the function `dnvmix()` internally approximates a $1$ dimensional integral using a randomized Quasi Monte-Carlo method. Due to the random nature, the result depends (slightly) on `.Random.seed`. Note that if $\Sigma$ is singular, the density is not defined.

Note the argument `log` in `dnvmix()`: Rather than estimating the density, internally the *log* density is estimated and only if `log = FALSE` (the default), the actual density is returned. This is usually more stable than estimating the density and then applying the $\log()$ function; also note that for many applications, interest actually lies in the log density, for instance when approximating the log-likelihood. 

We give a small example using $W$ such that $P(W=1)=0.5$, $P(W=5)=0.1$ and $P(W=10)=0.4$. 

```{r}
## we use qW from before
## Evaluate at the following 3 points simulatenously:
x <- matrix(1:15/15, ncol = d) 

## Density
set.seed(1)
(d1 <- dnvmix(x, qmix = qW, scale = P))

## Log Density
set.seed(1)
(d2 <- dnvmix(x, qmix = qW, scale = P, log = TRUE))

stopifnot(all.equal(d1, exp(d2), check.attributes = FALSE))
```

The error estimate given is the *worst case* error for all the inputs, i.e. the error of the row with the largest error. Note also how `error` is much smaller than the default tolerance $10^{-3}$: This is due to the fact that `dnvmix()` iterates until the desired precision is reached for *both*, the log - density and the density. Furthermore, the error estimate for `log = FALSE` is conservative - it will only differ from the error when `log = TRUE` if at least one density estimate is larger than 1.  


## 3 (Quasi-) Random Number Generation

The function `rnvmix` provides a flexible tool to sample from (multivariate) normal variance mixtures. The structure is similar to the one of `dnvmix()` and `pnvmix()`. The user can specify the argument `qmix` (which, as usual, corresponds to the quantile function of $W$) or, alternatively, the argument `rmix`, which corresponds to a random number generator for $W$. The advantage is that there may be distributions for which it is hard to find the quantile function, but for which sampling (e.g. by acceptance/rejection or stochastic representations) is easy. 

Consider the first example, where $W\sim Exp(1.9)$:
```{r, fig.align = "center", fig.width = 6, fig.height = 6, fig.show = "hold"}
## Sample size
n <- 500 

## We use P = diag(2), which is the default
set.seed(42)
r1 <- rnvmix(n, rmix = list("exp", rate = rate))
plot(r1, xlab = "X1", ylab = "X2")
```

Another important argument of `rnvmix` is `method`. This can be either `"PRNG"` for classical random sampling or `"sobol"` or `"ghalton"` which corresponds to inversion of a low-discrepancy pointset. If `method` is not `PRNG`, `qmix` must be provided. 

Let us revisit the example from before, where $W$ was discrete.

```{r, fig.align = "center", fig.width = 9, fig.height = 6, fig.show = "hold"}
r2 <- rnvmix(n, qmix = qW)
r3 <- rnvmix(n, qmix = qW, method = "ghalton")
par(mfrow=c(1,2))
plot(r2, xlab = "X1", ylab = "X2", main = "Pseudo-random sample")
plot(r3, xlab = "X1", ylab = "X2", main = "Quasi-random sample")
```


Unlike `pnvmix()` and `dnvmix()`, `rnvmix()` can handle singular normal variance mixtures. In this case, the matrix `factor` (which is a matrix $A$ such that $AA^T=\Sigma$) has to be provided. In the following example, we consider a $t$ distribution; here we can use the wrapper `rStudent()`:
```{r, fig.align = "center", fig.width = 9, fig.height = 6, fig.show = "hold"}
## Specify degrees of freedom
df <- 3.5 

## Specify factor
factor <- matrix( c(1,0,0,1,0,1), ncol = 2, byrow = TRUE)

## The corresponding 'scale' matrix is then
tcrossprod(factor)

## Now sample:
r4 <- rStudent(n, df = df, factor = factor)
#persp(x= r4[,1], y = r4[,2], z = r4[,3])

X1.=as.data.frame(r4)
#plot_ly(X1.,x=X1.[,1],y=X1.[,2],z=X1.[,3],type="scatter3d",mode='markers',opacity=0.3,name="Fibonacci lattice")
scatterplot3d(r4[,1], r4[,2], r4[,3])
```
